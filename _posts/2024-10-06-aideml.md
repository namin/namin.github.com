---
layout: post
title: AIDE on Ollama (a quick log)
---

AIDE ([code](https://github.com/WecoAI/aideml), [blog](https://www.weco.ai/blog/technical-report)) is an "LLM agent that generates solutions for machine learning tasks".

I enjoyed playing with AIDE.
Its codebase inspires me as a starting point (among others!)
to explore design and implementation of problem-solving agents.

So, I wanted to quickly document how I ran the AIDE README example on Ollama
(spec: on a maxed out Apple Silicon M3 MacBook Pro 2024),
with just some minor tweaks ([diff](https://github.com/namin/aideml/compare/before-my...namin:aideml:my)).
I used the [qwen2.5](https://www.ollama.com/library/qwen2.5) for all models (code, feedback, report).
The feedback model requires OpenAI tool support, and crucially, Ollama added tool support in July 2024 ([blog](https://ollama.com/blog/tool-support)).
Thus, AIDE supports Ollama out of the box via its OpenAI-compatible server ([blog](https://ollama.com/blog/openai-compatibility)).
On some runs, I got an assertion failure on the AIDE OpenAI backend ([code](https://github.com/namin/aideml/blob/my/aide/backend/backend_openai.py#L51-L53)).

## Running my experiment

- Install aideml from source using [`my`](https://github.com/namin/aideml/tree/my) branch in a fresh env with Python _3.10_ (NB: 3.12 failed for me with a Cython compilation error).
  ```
  git clone https://github.com/namin/aideml.git
  cd aideml
  conda create -n aideml python=3.10
  git checkout my
  pip install -e .
  ```
- Use the Ollama OpenAI-compatible server:
  `export OPENAI_BASE_URL="http://localhost:11434/v1"`
- Use the dummy OpenAI key:
  `export OPENAI_API_KEY="ollama"`
- Run the README example, explicitly setting each LLM model:
  ```aide report.model="qwen2.5" agent.code.model="qwen2.5" agent.feedback.model="qwen2.5" data_dir="example_tasks/house_prices" goal="Predict the sales price for each house" eval="Use the RMSE metric between the logarithm of the predicted and observed values."```

## In-Progress Screenshot

![screenshot of terminal while aideml is running](./assets/aideml/screenshot.png)

## Generated files

- [Python code file for the _best_ solution](./assets/aideml/best_solution.py)
- [**neat** visualization of solution tree (incrementally updated during run)](./assets/aideml/tree_plot.html)
- [journal.json](./assets/aideml/journal.json) (unfortunately, all the solutions are deemed buggy!)
- [final report from journal](./assets/aideml/report.html) (disclaimer: `my` branch configures and enables the final report generation which was already implemented but commented out in `main` branch)
- [config.yaml](./assets/aideml/config.yaml)
